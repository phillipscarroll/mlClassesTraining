{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "TRAIN_GOOD_FILE_COUNT = 250\n",
    "TEST_GOOD_FILE_COUNT = 50\n",
    "TRAIN_BAD_FILE_COUNT = 250\n",
    "TEST_BAD_FILE_COUNT = 50\n",
    "DATA_FOLDER = \"data\"\n",
    "WDSDataset_FOLDER = f\"{DATA_FOLDER}/benchmark_dataset\"\n",
    "\n",
    "# Generate Good files\n",
    "good_files = []\n",
    "for i in range(TRAIN_GOOD_FILE_COUNT + TEST_GOOD_FILE_COUNT):\n",
    "    data = {\n",
    "        \"id\": random.randint(100000, 999999),\n",
    "        \"name\": {\n",
    "            \"first\": f\"John\",\n",
    "            \"last\": f\"Doe {i+1}\"\n",
    "        },\n",
    "        \"age\": random.randint(18, 60),\n",
    "        \"score\": {\n",
    "            \"math\": round(random.uniform(0.6, 1.0), 2),\n",
    "            \"science\": round(random.uniform(0.6, 1.0), 2),\n",
    "            \"english\": round(random.uniform(0.6, 1.0), 2)\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"street\": f\"{random.randint(100, 999)} Main St\",\n",
    "            \"city\": \"Anytown\",\n",
    "            \"state\": \"CA\",\n",
    "            \"zip\": f\"{random.randint(10000, 99999)}\"\n",
    "        },\n",
    "        \"contacts\": [\n",
    "            {\"type\": \"email\", \"value\": f\"john.doe{i+1}@example.com\"},\n",
    "            {\"type\": \"phone\", \"value\": f\"555-{random.randint(1000, 9999)}\"}\n",
    "        ]\n",
    "    }\n",
    "    good_files.append(json.dumps(data))\n",
    "\n",
    "# Generate Bad files\n",
    "bad_files = []\n",
    "for i in range(TRAIN_BAD_FILE_COUNT + TEST_BAD_FILE_COUNT):\n",
    "    data = {\n",
    "        \"id\": random.randint(100000, 999999),\n",
    "        \"name\": {\n",
    "            \"first\": f\"John\",\n",
    "            \"last\": f\"Doe {i+1}\"\n",
    "        },\n",
    "        \"age\": random.randint(18, 60),\n",
    "        \"score\": {\n",
    "            \"math\": round(random.uniform(0.4, 0.9), 2),\n",
    "            \"science\": round(random.uniform(0.4, 0.9), 2),\n",
    "            \"english\": round(random.uniform(0.4, 0.9), 2)\n",
    "        },\n",
    "        \"address\": {\n",
    "            \"street\": f\"{random.randint(100, 999)} Main St\",\n",
    "            \"city\": \"Anytown\",\n",
    "            \"state\": \"CA\",\n",
    "            \"zip\": f\"{random.randint(10000, 99999)}\"\n",
    "        },\n",
    "        \"contacts\": [\n",
    "            {\"type\": \"email\", \"value\": f\"john.doe{i+1}@example.com\"},\n",
    "            {\"type\": \"phone\", \"value\": f\"555-{random.randint(1000, 9999)}\"}\n",
    "        ],\n",
    "        \"feedback\": [\"bad\", \"not good\", \"block\"][random.randint(0, 2)]\n",
    "    }\n",
    "    bad_files.append(json.dumps(data))\n",
    "\n",
    "# Create train and test folder structure\n",
    "train_good_folder_path = f\"{WDSDataset_FOLDER}/train/good\"\n",
    "train_bad_folder_path = f\"{WDSDataset_FOLDER}/train/bad\"\n",
    "test_good_folder_path = f\"{WDSDataset_FOLDER}/test/good\"\n",
    "test_bad_folder_path = f\"{WDSDataset_FOLDER}/test/bad\"\n",
    "\n",
    "os.makedirs(train_good_folder_path, exist_ok=True)\n",
    "os.makedirs(train_bad_folder_path, exist_ok=True)\n",
    "os.makedirs(test_good_folder_path, exist_ok=True)\n",
    "os.makedirs(test_bad_folder_path, exist_ok=True)\n",
    "\n",
    "# Save Good files to train/GOOD and test/GOOD folders\n",
    "for i in range(TRAIN_GOOD_FILE_COUNT):\n",
    "    good_file_path = f\"{train_good_folder_path}/good{i+1}.json\"\n",
    "    with open(good_file_path, \"w\") as f:\n",
    "        f.write(good_files[i])\n",
    "\n",
    "for i in range(TEST_GOOD_FILE_COUNT):\n",
    "    good_file_path = f\"{test_good_folder_path}/good{i+1}.json\"\n",
    "    with open(good_file_path, \"w\") as f:\n",
    "        f.write(good_files[TRAIN_GOOD_FILE_COUNT + i])\n",
    "\n",
    "# Save Bad files to train/BAD and test/BAD folders\n",
    "for i in range(TRAIN_BAD_FILE_COUNT):\n",
    "    bad_file_path = f\"{train_bad_folder_path}/bad{i+1}.json\"\n",
    "    with open(bad_file_path, \"w\") as f:\n",
    "        f.write(bad_files[i])\n",
    "\n",
    "for i in range(TEST_BAD_FILE_COUNT):\n",
    "    bad_file_path = f\"{test_bad_folder_path}/bad{i+1}.json\"\n",
    "    with open(bad_file_path, \"w\") as f:\n",
    "        f.write(bad_files[TRAIN_BAD_FILE_COUNT + i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Function to set the number of CPU threads\n",
    "def set_cpu_thread_cap(num_threads):\n",
    "    torch.set_num_threads(num_threads)\n",
    "    torch.set_num_interop_threads(num_threads)\n",
    "    print(f\"Set max CPU threads to: {num_threads}\")\n",
    "\n",
    "# Set the device type and device ID\n",
    "device_type = \"xpu\" if torch.xpu.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_id = 0  # Change this to the desired device index, e.g., 0 or 1\n",
    "# Combine device type and device ID to create the device variable\n",
    "device = torch.device(f\"{device_type}:{device_id}\" if device_type else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# If running on CPU, set the number of threads\n",
    "if device == 'cpu':\n",
    "    set_cpu_thread_cap(20)  # Set this to the desired number of CPU threads\n",
    "\n",
    "class JSONDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, max_length=512):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        for label in ['good', 'bad']:\n",
    "            label_dir = os.path.join(data_dir, label)\n",
    "            for file_name in os.listdir(label_dir):\n",
    "                if file_name.endswith('.json'):\n",
    "                    file_path = os.path.join(label_dir, file_name)\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        script_data = json.load(f)\n",
    "                        # Convert JSON data to a string (simple approach, can be improved)\n",
    "                        text = str(script_data)\n",
    "                        self.data.append((text, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(1 if label == 'good' else 0, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dir = 'data/benchmark_dataset/train'\n",
    "test_dir = 'data/benchmark_dataset/test'\n",
    "\n",
    "train_dataset = JSONDataset(train_dir, tokenizer)\n",
    "test_dataset = JSONDataset(test_dir, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Initialize GradScaler for mixed precision training\n",
    "scaler = torch.GradScaler()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(\"xpu\"):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast('xpu'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=['bad', 'good'])\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "benchmarkLoops = 10\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the training loop with tqdm for progress bar\n",
    "for i in tqdm.tqdm(range(benchmarkLoops)):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler)\n",
    "        accuracy, report = test_model(model, test_loader)\n",
    "        print(f\"Loop {i+1} - Epoch {epoch+1} - Train Loss: {train_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
